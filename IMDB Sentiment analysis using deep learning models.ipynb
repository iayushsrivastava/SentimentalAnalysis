{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715},{"sourceId":816060,"sourceType":"datasetVersion","datasetId":429163}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n## Installing and importing dependencies\n\nFirst let us import all the modules and packages that will be required.\n","metadata":{}},{"cell_type":"code","source":"# A dependency of the preprocessing for BERT inputs\n!pip data.shapeinstall -q tensorflow-text","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:24:36.012902Z","iopub.execute_input":"2023-12-17T01:24:36.013488Z","iopub.status.idle":"2023-12-17T01:24:37.565874Z","shell.execute_reply.started":"2023-12-17T01:24:36.013455Z","shell.execute_reply":"2023-12-17T01:24:37.564936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q tf-models-official","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:24:42.813665Z","iopub.execute_input":"2023-12-17T01:24:42.814025Z","iopub.status.idle":"2023-12-17T01:25:50.601457Z","shell.execute_reply.started":"2023-12-17T01:24:42.813996Z","shell.execute_reply":"2023-12-17T01:25:50.600483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nfrom official.nlp import optimization  # to create AdamW optimizer\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n#for visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:26:05.432105Z","iopub.execute_input":"2023-12-17T01:26:05.432972Z","iopub.status.idle":"2023-12-17T01:26:11.471402Z","shell.execute_reply.started":"2023-12-17T01:26:05.432937Z","shell.execute_reply":"2023-12-17T01:26:11.470434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading and Exploring dataset\n\n\n\nThere is one file provided to us in this dataset:\n\n * `IMDB-Dataset.csv`: The CSV file containing all the reviews and their polarity.\n \nLet's read the file.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndata_imdb =pd.read_csv(r'../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:26:18.352192Z","iopub.execute_input":"2023-12-17T01:26:18.353431Z","iopub.status.idle":"2023-12-17T01:26:19.794055Z","shell.execute_reply.started":"2023-12-17T01:26:18.353387Z","shell.execute_reply":"2023-12-17T01:26:19.793042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thus there are 50000 rows and 2 columns in the `data` dataframe ","metadata":{}},{"cell_type":"code","source":"data_imdb.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:26:22.42721Z","iopub.execute_input":"2023-12-17T01:26:22.42756Z","iopub.status.idle":"2023-12-17T01:26:22.446857Z","shell.execute_reply.started":"2023-12-17T01:26:22.427532Z","shell.execute_reply":"2023-12-17T01:26:22.445852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see the `sentiment` column has 2 values:\n\n   * Negative Sentiment\n   * Postive Sentiment\n  ","metadata":{}},{"cell_type":"markdown","source":"Now we encode the positive sentiment to 1 and negative sentiment to 0","metadata":{}},{"cell_type":"code","source":"labeling = {\n    'positive':1, \n    'negative':0\n}\n\ndata_imdb['sentiment'] = data_imdb['sentiment'].apply(lambda x : labeling[x])\n# Output first ten rows\ndata_imdb.head(10)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:26:28.595591Z","iopub.execute_input":"2023-12-17T01:26:28.595995Z","iopub.status.idle":"2023-12-17T01:26:28.64038Z","shell.execute_reply.started":"2023-12-17T01:26:28.595966Z","shell.execute_reply":"2023-12-17T01:26:28.639468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thus, there are no missing values in any of the columns of the dataset.","metadata":{}},{"cell_type":"code","source":"label=data_imdb['sentiment']\ndata_imdb=data_imdb.drop(['sentiment'],axis=1)\nlabel=label.tolist()\nprint(type(label))","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:26:35.396769Z","iopub.execute_input":"2023-12-17T01:26:35.397139Z","iopub.status.idle":"2023-12-17T01:26:35.412693Z","shell.execute_reply.started":"2023-12-17T01:26:35.39711Z","shell.execute_reply":"2023-12-17T01:26:35.411476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_imdb=data_imdb['review'].tolist()\nprint(type(data_imdb))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:26:37.643835Z","iopub.execute_input":"2023-12-17T01:26:37.64461Z","iopub.status.idle":"2023-12-17T01:26:37.652434Z","shell.execute_reply.started":"2023-12-17T01:26:37.644578Z","shell.execute_reply":"2023-12-17T01:26:37.651429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=data_imdb\nlabels=label","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:26:40.570983Z","iopub.execute_input":"2023-12-17T01:26:40.571386Z","iopub.status.idle":"2023-12-17T01:26:40.575758Z","shell.execute_reply.started":"2023-12-17T01:26:40.571355Z","shell.execute_reply":"2023-12-17T01:26:40.574711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 500 # 句子 上限200个词\nEMBEDDING_DIM = 10\n# https://keras-cn-docs.readthedocs.io/zh_CN/latest/blog/word_embedding/\nimport numpy as np\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# tokenizer\ntexts = data\ntokenizer = Tokenizer(char_level=True) # 字向量\ntokenizer.fit_on_texts(texts)\nword_index = tokenizer.word_index\n\n# sequences\nsequences = tokenizer.texts_to_sequences(data)\n\n# padding\ndata = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n\nprint('Found %s unique tokens.' % len(word_index))\nprint('Shape of data tensor:', data.shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:26:43.148584Z","iopub.execute_input":"2023-12-17T01:26:43.149557Z","iopub.status.idle":"2023-12-17T01:27:22.433281Z","shell.execute_reply.started":"2023-12-17T01:26:43.149524Z","shell.execute_reply":"2023-12-17T01:27:22.432312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\n# 打乱顺序\nindex = [i for i in range(len(data))]\nrandom.shuffle(index)\ndata = np.array(data)[index]\nlabels = np.array(labels)[index]\n\nTRAIN_SPLIT = 0.8 # 20% 测试集\nTRAIN_SIZE = int(len(data) * TRAIN_SPLIT)\n\nX_train, X_test = data[0:TRAIN_SIZE], data[TRAIN_SIZE:]\nY_train, Y_test = labels[0:TRAIN_SIZE], labels[TRAIN_SIZE:]","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:27:26.714942Z","iopub.execute_input":"2023-12-17T01:27:26.715799Z","iopub.status.idle":"2023-12-17T01:27:26.870381Z","shell.execute_reply.started":"2023-12-17T01:27:26.715759Z","shell.execute_reply":"2023-12-17T01:27:26.86935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CNN+BILSTM\n\n","metadata":{}},{"cell_type":"code","source":"\nfrom keras.models import Sequential\nfrom keras.layers import Activation, BatchNormalization\nfrom keras.layers import Dense, LSTM, Convolution1D, MaxPooling1D\nfrom keras.layers import Embedding\nfrom keras.layers import Bidirectional\n\n\nQA_EMBED_SIZE = 64\nDROPOUT_RATE = 0.3\n\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\nmodel.add(Convolution1D(filters=128, kernel_size=3, padding='valid', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling1D(4))\nmodel.add(Bidirectional(LSTM(QA_EMBED_SIZE, return_sequences=False, dropout=DROPOUT_RATE, recurrent_dropout=DROPOUT_RATE)))\n\nmodel.add(Dense(QA_EMBED_SIZE))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dense(1))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"sigmoid\"))\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:27:35.105938Z","iopub.execute_input":"2023-12-17T01:27:35.106867Z","iopub.status.idle":"2023-12-17T01:27:35.935443Z","shell.execute_reply.started":"2023-12-17T01:27:35.106824Z","shell.execute_reply":"2023-12-17T01:27:35.934511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try the preprocessing model on sample text and see the output:","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom keras import backend as K\n\ndef precision(y_true, y_pred):\n    \"\"\"Precision metric.\n\n    Only computes a batch-wise average of precision.\n\n    Computes the precision, a metric for multi-label classification of\n    how many selected items are relevant.\n    \"\"\"\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef recall(y_true, y_pred):\n    \"\"\"Recall metric.\n\n    Only computes a batch-wise average of recall.\n\n    Computes the recall, a metric for multi-label classification of\n    how many relevant items are selected.\n    \"\"\"\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:27:57.439566Z","iopub.execute_input":"2023-12-17T01:27:57.439941Z","iopub.status.idle":"2023-12-17T01:27:57.454352Z","shell.execute_reply.started":"2023-12-17T01:27:57.439914Z","shell.execute_reply":"2023-12-17T01:27:57.453064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n# from keras.utils import multi_gpu_model\n#from evaluate import *\n\nEPOCHS = 3\nBATCH_SIZE = 64 \nVALIDATION_SPLIT = 0.3 # 30% 验证集\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10)\nmodel_checkpoint = ModelCheckpoint('model/model-cnn-blstm.h5', save_best_only=True, save_weights_only=True)\ntensor_board = TensorBoard('log/tflog-cnn-blstm', write_graph=True, write_images=True)\n\n# model = multi_gpu_model(model)\n\nmodel.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy', precision, recall, f1])\n\nmodel.fit(X_train, Y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, \n          validation_split=VALIDATION_SPLIT, shuffle=True, \n          callbacks=[early_stopping, model_checkpoint, tensor_board])","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:28:11.950387Z","iopub.execute_input":"2023-12-17T01:28:11.950885Z","iopub.status.idle":"2023-12-17T01:35:09.889671Z","shell.execute_reply.started":"2023-12-17T01:28:11.95085Z","shell.execute_reply":"2023-12-17T01:35:09.888641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(X_test, Y_test, verbose=1, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:35:19.165028Z","iopub.execute_input":"2023-12-17T01:35:19.165588Z","iopub.status.idle":"2023-12-17T01:35:25.63641Z","shell.execute_reply.started":"2023-12-17T01:35:19.16556Z","shell.execute_reply":"2023-12-17T01:35:25.635331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CNN+BILSTM+CNN","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Activation, BatchNormalization, Flatten\nfrom keras.layers import Dense, LSTM, Convolution1D, MaxPooling1D\nfrom keras.layers import Embedding\nfrom keras.layers import Bidirectional\n\nQA_EMBED_SIZE = 64\nDROPOUT_RATE = 0.3\n\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\nmodel.add(Convolution1D(filters=128, kernel_size=3, padding='valid', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling1D(4))\nmodel.add(Bidirectional(LSTM(QA_EMBED_SIZE, return_sequences=True, dropout=DROPOUT_RATE, recurrent_dropout=DROPOUT_RATE)))\nmodel.add(Convolution1D(filters=128, kernel_size=3, padding='valid', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling1D(4))\nmodel.add(Flatten())\n\nmodel.add(Dense(QA_EMBED_SIZE))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dense(1))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"sigmoid\"))\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:35:38.558369Z","iopub.execute_input":"2023-12-17T01:35:38.559125Z","iopub.status.idle":"2023-12-17T01:35:39.058597Z","shell.execute_reply.started":"2023-12-17T01:35:38.559085Z","shell.execute_reply":"2023-12-17T01:35:39.057626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n# from keras.utils import multi_gpu_model\n#from evaluate import *\n\nEPOCHS = 3\nBATCH_SIZE = 64 \nVALIDATION_SPLIT = 0.3 # 30% 验证集\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10)\nmodel_checkpoint = ModelCheckpoint('model/model-cnn-blstm-cnn.h5', save_best_only=True, save_weights_only=True)\ntensor_board = TensorBoard('log/tflog-cnn-blstm-cnn', write_graph=True, write_images=True)\n\n# model = multi_gpu_model(model)\n\nmodel.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy', precision, recall, f1])\n\nmodel.fit(X_train, Y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, \n          validation_split=VALIDATION_SPLIT, shuffle=True, \n          callbacks=[early_stopping, model_checkpoint, tensor_board])","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:35:44.640287Z","iopub.execute_input":"2023-12-17T01:35:44.640641Z","iopub.status.idle":"2023-12-17T01:43:25.736039Z","shell.execute_reply.started":"2023-12-17T01:35:44.640612Z","shell.execute_reply":"2023-12-17T01:43:25.734878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(X_test, Y_test, verbose=1, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:54:52.754898Z","iopub.execute_input":"2023-12-17T01:54:52.755286Z","iopub.status.idle":"2023-12-17T01:55:02.136519Z","shell.execute_reply.started":"2023-12-17T01:54:52.755257Z","shell.execute_reply":"2023-12-17T01:55:02.135497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BILSTM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Activation, BatchNormalization\nfrom keras.layers import Dense, LSTM\nfrom keras.layers import Embedding\nfrom keras.layers import Bidirectional\n\nQA_EMBED_SIZE = 64\nDROPOUT_RATE = 0.3\n\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\nmodel.add(Bidirectional(LSTM(QA_EMBED_SIZE, return_sequences=False, dropout=DROPOUT_RATE, recurrent_dropout=DROPOUT_RATE)))\n\nmodel.add(Dense(QA_EMBED_SIZE))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dense(1))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"sigmoid\"))\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:56:17.680878Z","iopub.execute_input":"2023-12-17T01:56:17.681562Z","iopub.status.idle":"2023-12-17T01:56:18.06588Z","shell.execute_reply.started":"2023-12-17T01:56:17.681528Z","shell.execute_reply":"2023-12-17T01:56:18.064913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n# from keras.utils import multi_gpu_model\n#from evaluate import *\n\nEPOCHS = 3\nBATCH_SIZE = 64 \nVALIDATION_SPLIT = 0.3 # 30% 验证集\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10)\nmodel_checkpoint = ModelCheckpoint('model/model-blstm.h5', save_best_only=True, save_weights_only=True)\ntensor_board = TensorBoard('log/tflog-blstm', write_graph=True, write_images=True)\n\n# model = multi_gpu_model(model)\n\nmodel.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy', precision, recall, f1])\n\nmodel.fit(X_train, Y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, \n          validation_split=VALIDATION_SPLIT, shuffle=True, \n          callbacks=[early_stopping, model_checkpoint, tensor_board])","metadata":{"execution":{"iopub.status.busy":"2023-12-17T01:56:21.266545Z","iopub.execute_input":"2023-12-17T01:56:21.266888Z","iopub.status.idle":"2023-12-17T02:13:12.255438Z","shell.execute_reply.started":"2023-12-17T01:56:21.266864Z","shell.execute_reply":"2023-12-17T02:13:12.254413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(X_test, Y_test, verbose=1, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T02:13:12.257424Z","iopub.execute_input":"2023-12-17T02:13:12.258117Z","iopub.status.idle":"2023-12-17T02:13:27.796998Z","shell.execute_reply.started":"2023-12-17T02:13:12.258079Z","shell.execute_reply":"2023-12-17T02:13:27.79614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BiLSTM+CNN","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Activation, BatchNormalization, Flatten\nfrom keras.layers import Dense, LSTM, Convolution1D, MaxPooling1D\nfrom keras.layers import Embedding\nfrom keras.layers import Bidirectional\n\nQA_EMBED_SIZE = 64\nDROPOUT_RATE = 0.3\n\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\nmodel.add(Bidirectional(LSTM(QA_EMBED_SIZE, return_sequences=True, dropout=DROPOUT_RATE, recurrent_dropout=DROPOUT_RATE)))\nmodel.add(Convolution1D(filters=128, kernel_size=3, padding='valid', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling1D(4))\nmodel.add(Flatten())\n\nmodel.add(Dense(QA_EMBED_SIZE))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dense(1))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"sigmoid\"))\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T02:15:29.262621Z","iopub.execute_input":"2023-12-17T02:15:29.263064Z","iopub.status.idle":"2023-12-17T02:15:29.694967Z","shell.execute_reply.started":"2023-12-17T02:15:29.263035Z","shell.execute_reply":"2023-12-17T02:15:29.693418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n# from keras.utils import multi_gpu_model\n#from evaluate import *\n\nEPOCHS = 3\nBATCH_SIZE = 64 \nVALIDATION_SPLIT = 0.3 # 30% 验证集\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10)\nmodel_checkpoint = ModelCheckpoint('model/model-blstm-cnn.h5', save_best_only=True, save_weights_only=True)\ntensor_board = TensorBoard('log/tflog-blstm-cnn', write_graph=True, write_images=True)\n\n# model = multi_gpu_model(model)\n\nmodel.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy', precision, recall, f1])\n\nmodel.fit(X_train, Y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, \n          validation_split=VALIDATION_SPLIT, shuffle=True, \n          callbacks=[early_stopping, model_checkpoint, tensor_board])","metadata":{"execution":{"iopub.status.busy":"2023-12-17T02:15:39.121429Z","iopub.execute_input":"2023-12-17T02:15:39.121882Z","iopub.status.idle":"2023-12-17T02:36:39.866437Z","shell.execute_reply.started":"2023-12-17T02:15:39.121827Z","shell.execute_reply":"2023-12-17T02:36:39.865644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(X_test, Y_test, verbose=1, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T02:36:39.868036Z","iopub.execute_input":"2023-12-17T02:36:39.868326Z","iopub.status.idle":"2023-12-17T02:36:59.424756Z","shell.execute_reply.started":"2023-12-17T02:36:39.868301Z","shell.execute_reply":"2023-12-17T02:36:59.423828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}